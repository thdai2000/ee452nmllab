{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [NML-25] Notebook 5: Graph Neural Networks\n",
        "\n",
        "TAs: [Sevda Öğüt](https://people.epfl.ch/sevda.ogut)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "**Expected output:**\n",
        "\n",
        "You will have coding and theoretical questions. Coding exercises shall be solved within the specified space:\n",
        "```python\n",
        "# Your solution here ###########################################################\n",
        "...\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "```\n",
        "Sometimes we provide variable names, such as `x = ...`; do not change names and stick to hinted typing, as they will be reused later.\n",
        "Within the solution space, you can declare any other variable or function that you might need, but anything outside these lines shall not be changed, or it will invalidate your answers.\n",
        "\n",
        "Theoretical questions shall be answered in the following markdown cell. The first line will be\n",
        "```markdown\n",
        "**Your answer here:**\n",
        "...\n",
        "```\n",
        "\n",
        "**Solutions:**\n",
        "* Your code should be self-contained in the `.ipynb` file. The solution to the exercices will be provided in an external `.ipynb` file.\n",
        "\n",
        "* Try to make your code clean and readable, it is a good training for the project. Provide meaningful variable names and comment where needed.\n",
        "\n",
        "* You cannot import any other library than we imported, unless explicitly stated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmp4-YzspA2V"
      },
      "source": [
        "## Objective\n",
        "\n",
        "This assignment focuses on Graph Neural Networks.\n",
        "In the first part, you will load and prepare data using the PyTorch Geometric library, define a GNN, implement train and test functions, and comment on the results.\n",
        "In the second part, you will define a new GNN and include it in the previous architecture.\n",
        "\n",
        "Please make sure you install the packages in `requirements.txt` respective to this notebook in your environment. Make sure that your Python version is not greater than 3.11!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = torch.__version__.split('+')[1]\n",
        "print(f\"PyTorch version: {TORCH}\")\n",
        "print(f\"CUDA version: {CUDA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that wheels are currently not available for M1/M2/M3 macs. Please install the extension packages (pyg_lib and torch_sparse) from source if you have mac. See the [installation documentation](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) for details!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pyg_lib torch_sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "The additional [tutorial](gdl_tutorial.ipynb) provides a broad overview of PyTorch and PyTorch Geometric, showing how to manipulate tensors and train neural networks as well as graph neural networks.\n",
        "\n",
        "The following resources might help you familiarize with PyTorch and PyTorch geometric.\n",
        "\n",
        "* [PyTorch: Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "* [PyTorch Geometric: Official Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html#official-examples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv2QsxmdLWNN"
      },
      "source": [
        "## Section 0: Explore the data\n",
        "\n",
        "In this section, we will go through the data to get a feeling of its content. We work with the [GitHub dataset](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.GitHub.html), from the [Multi-scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) paper. In this dataset, nodes represent developers on GitHub and edges are mutual follower relationships. It contains 37 300 nodes, 578 006 edges, 128-dimensional node features and 2 classes.\n",
        "\n",
        "This dataset is readily available in PyTorch Geometric, so let's import relevant libraries and then install the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-ZfCASHUyyK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as pyg\n",
        "from scipy import sparse\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch_geometric.datasets import GitHub\n",
        "from torchmetrics import Metric\n",
        "from torchmetrics.classification import Accuracy, BinaryF1Score, Precision, Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0iyzAIJkMRAR"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PASnfjHmG5WN"
      },
      "outputs": [],
      "source": [
        "dataset = GitHub(\".\")\n",
        "data = dataset._data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3YM75iqHExP"
      },
      "source": [
        "Now, we shall study its content. Node attributes are accessible through the `x` attribute, which is a `torch.Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EojTx0W9HEWB"
      },
      "outputs": [],
      "source": [
        "print(\"Design matrix\")\n",
        "n_nodes, n_feats = data.x.shape\n",
        "print(f\"Num. nodes: {n_nodes}; num features: {n_feats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkxKqIAlHcfn"
      },
      "source": [
        "We see that we have 37 700 nodes, each with 128 features. The features correspond to an embedding of location, starred repositories, employer, and e-mail address information of each user. Each node comes with a binary label, which indicates whether it corresponds to a web or a machine learning developer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pADYpCsfILUX"
      },
      "outputs": [],
      "source": [
        "print(\"Target vector\")\n",
        "print(\"First five elements:\", data.y[:5])\n",
        "print(\"Number of samples:\", data.y.shape)\n",
        "print(\"Number of nodes in class 1:\", data.y.sum().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMWyuV57lN-v"
      },
      "source": [
        "We see that the task is quite imbalanced, as class 1 is underrepresented. To get more meaningful interpretations, we swap classes zero and one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r96967dQJ909"
      },
      "outputs": [],
      "source": [
        "data.y = 1 - data.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8tddwFBIKls"
      },
      "source": [
        "The edges are contained in the `edge_index` attribute, which is again a tensor. Let's check its shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvC2cdChIKV_",
        "outputId": "c71ed1cd-1388-4639-8524-0abcff590fdd"
      },
      "outputs": [],
      "source": [
        "print(\"Edge index shape:\", data.edge_index.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIeGm9pZOIeU"
      },
      "source": [
        "Describe the content of the edge index matrix and how it relates to the adjacency matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O29ZREuOwS1"
      },
      "source": [
        "**Your answer here:**\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0VxhCEZlj3A"
      },
      "source": [
        "Now, we will create two random binary masks on the nodes: one for training and one for testing. We would like to have 80% of the samples in the training split, so we will uniformly pick nodes with that probability.\n",
        "\n",
        "We use a masking strategy instead of directly splitting the data because our interpretation of the task is that we have a social network in which the training labels are accessible, while the test nodes, even though available, are unknown. This simplifies the sampling strategy, in particular for network methods, as we do not have to worry about loosing structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci3Hoxz-ljdU",
        "outputId": "2d8aef5b-e38f-4cf2-f34b-9ce4e7bb7fff"
      },
      "outputs": [],
      "source": [
        "rng = torch.Generator().manual_seed(452)\n",
        "train_mask = torch.randn(n_nodes, generator=rng) < 0.8\n",
        "\n",
        "n_nodes_tr = train_mask.sum().item()\n",
        "print(f\"Training set size: {n_nodes_tr} ({n_nodes_tr / n_nodes:.2%})\")\n",
        "print(f\"Test set size: {n_nodes - n_nodes_tr} ({1 - n_nodes_tr / n_nodes:.2%})\")\n",
        "print(\n",
        "    f\"Ratio of class 1 in training: {torch.sum(train_mask * data.y).item() / n_nodes_tr:.2%}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hIHBZWO_E2"
      },
      "source": [
        "Note that a graph with 37 700 nodes yields a 37 700 × 37 700 matrix, which has approximately 1 421 290 000 entries. Suppose that this graph is stored with a **dense** representation with 1 bit per entry, then this would result in ~170 MB of storage space. If 8 bits (1 byte) are used per entry, then this would give us ~1.42 GB. Since PyTorch uses 64-bit floats (8 bytes) per entry, we would get more than 10 GB for this graph. This illustrates that even though many entries may be zero (meaning no edge) in this 37 700 x 37 700 matrix, a dense representation allocates space for all possible node pairs. However, in a **sparse** representation, instead of storing all possible entries, only the existing edges (nonzero entries) are stored, which  can significantly reduce the required space (if you do the math it's ~2.2 MB in this example) 😄"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TTeSCNpAhNL"
      },
      "source": [
        "## Section 1: Deep learning on graph data\n",
        "\n",
        "This part presents a general workflow for deep learning and our recommended libraries: PyTorch and PyTorch Geometric. We will start with classical ML baselines, to get some robust results to which we can compare. Then we will introduce graph features, to see whether they can help in our task. Finally, we will work with deep learning and graph neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kyFm7htUGtx"
      },
      "source": [
        "### Question 1.1: Baseline\n",
        "\n",
        "In this question we define a baseline model with a classical ML method, namely a random forest, to get an idea of what performances we can expect from the following models. This model will only use node features, so it does not leverage at all the graph structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GutJnum0js4K"
      },
      "source": [
        "**1.1.1** Train a random forest classifier based on the node features. Make sure to use the provided `train_mask` for both the features and the target labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtxCOzQgjsgV"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "rf_classifier = RandomForestClassifier().fit(\n",
        "    data.x[train_mask].numpy(), data.y[train_mask].numpy()\n",
        ")\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu-xUZHUkyhZ"
      },
      "source": [
        "**1.1.2** Predict the labels of the test nodes, then print the `classification_report`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09E56x7SkyR6",
        "outputId": "2affcdba-1ff9-4e25-8de8-2d8421825203"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "y_rf = rf_classifier.predict(data.x[~train_mask].numpy())\n",
        "\n",
        "print(classification_report(data.y[~train_mask].numpy(), y_rf))\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttmPz_DUlG4R"
      },
      "source": [
        "**1.1.2** Predict the labels of the test nodes, then print the `classification_report`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFUAHSG_lGcH"
      },
      "source": [
        "**Your answer here:**\n",
        "The most meaningful metric is the F1 score, and in particular the macro average, since the classes are very imbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBcITtgLRXQ"
      },
      "source": [
        "### Question 1.2: Graph baseline - Laplacian eigenmaps\n",
        "\n",
        "Now, let's implement a second benchmark, this time relying on structural properties. We would like to use eigenmaps of the Laplacian, but if you try to do it you would quickly run out of memory! (Go ahead and try if you wish 😉)\n",
        "\n",
        "The adjacency matrix is too big to use it in computations, but it would be mainly filled with zeros. We can optimize memory and running time by using a **sparse representation** as discussed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut_5nLpJW8Fy"
      },
      "source": [
        "**1.2.1** Compute the Laplacian matrix as a [sparse SciPy array][scipy_sparse]. Start by creating a sparse adjacency matrix form the `edge_index`, supposing that all edge weights are 1.\n",
        "\n",
        "[scipy_sparse]: https://docs.scipy.org/doc/scipy/reference/sparse.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUkPozOLXsU2"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "adjacency = sparse.coo_array(\n",
        "    (\n",
        "        np.ones(data.edge_index.shape[1]),\n",
        "        (data.edge_index[0].numpy(), data.edge_index[1].numpy()),\n",
        "    )\n",
        ")\n",
        "laplacian = sparse.diags(adjacency.sum(0)) - adjacency\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpcX9YzpdHhg"
      },
      "source": [
        "**1.2.2** Use SciPy sparse linear algebra capabilities to compute the first 5 nontrivial eigenvectors of the Laplacian.\n",
        "\n",
        "*Note: If this takes too long, you can change the condition to False after completing Question 1.2 to iterate more quickly over the following ones.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPl0GpFNXtqb"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "if False:  # Change to True to run cell\n",
        "\n",
        "    _eigvals, eigvecs = sparse.linalg.eigsh(\n",
        "        laplacian, k=6, which=\"SM\" #v0=np.ones(n_nodes)\n",
        "    )\n",
        "    eigvecs = eigvecs[:, 1:]\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "else:\n",
        "    eigvecs = np.random.rand(n_nodes, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSA1eROhf6C0"
      },
      "source": [
        "**1.2.3** Train and test a new random forest classifier using the eigenvector representation as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9auF0OKHsQ6",
        "outputId": "c0fa19e4-cc95-44e2-c0b9-51b1da87269b"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "rf_eigrep = RandomForestClassifier().fit(\n",
        "    eigvecs[train_mask], data.y[train_mask].numpy()\n",
        ")\n",
        "\n",
        "y_rfe = rf_eigrep.predict(eigvecs[~train_mask])\n",
        "\n",
        "print(classification_report(data.y[~train_mask].numpy(), y_rfe))\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLmBT9l0IUh-"
      },
      "source": [
        "**1.2.4** Now, combine the two sets of features, i.e. the given programmers features and the Laplacian eigenmaps, into a single design matrix, then train and test another random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hS6Wk8rf769",
        "outputId": "8575254f-1487-421e-da12-5470386573cb"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "feats = np.concatenate([data.x.numpy(), eigvecs], axis=1)\n",
        "\n",
        "rf_combi = RandomForestClassifier().fit(feats[train_mask], data.y[train_mask].numpy())\n",
        "\n",
        "y_rfc = rf_combi.predict(feats[~train_mask])\n",
        "\n",
        "print(classification_report(data.y[~train_mask].numpy(), y_rfc))\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSH_KELJHTh"
      },
      "source": [
        "**1.2.5** Comment on the results and describe which model you expect to perform the best on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StE6dXwWXtxP"
      },
      "source": [
        "**Your answer here:**\n",
        "We see that the test macro-avg F1 score for the node features, eigen-representation, and combined model are 0.81, 0.79, and 0.82 respectively. The combined model seems to be the best one, as its improves both precision and recall for class 0.\n",
        "\n",
        "*Note: Results might change slightly due to random seeding!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRXIUrGNWpKe"
      },
      "source": [
        "### Question 1.3: Neural network baseline - MLP\n",
        "\n",
        "In this question, we move from classical ML to deep learning and, again, we start from a simple model to get a viable benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPBDUDIM-O2E"
      },
      "source": [
        "**1.3.1** Create two `DataLoaders`, for the training and test data respectively, by using the `TensorDataset` class. Use the predefined batch size and shuffle training data.\n",
        "\n",
        "References:\n",
        "- [Datasets and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "- [PyTorch data utility](https://pytorch.org/docs/stable/data.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH3G5rTBtp6D"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    TensorDataset(data.x[train_mask], data.y[train_mask]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "loader_test = DataLoader(\n",
        "    TensorDataset(data.x[~train_mask], data.y[~train_mask]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIHAOnGL9Sm"
      },
      "source": [
        "**1.3.2** Define a [module][nn_module] for a two-layer perceptron with the ReLU activation function. The hidden dimension will be a parameter of the constructor function.\n",
        "This neural network will take as input a design matrix and predict the \"logits\" of class 1.\n",
        "\n",
        "[nn_module]: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiyHwfHLgsIT"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    # Your solution here #######################################################\n",
        "    def __init__(self, in_features: int, hidden_features: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_features=in_features, out_features=hidden_features)\n",
        "        self.linear2 = nn.Linear(in_features=hidden_features, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x).relu()\n",
        "        return self.linear2(x)\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDJDlVGLtoTp"
      },
      "source": [
        "**1.3.3** Define a function to perform one training step for a given NN, taking as argument a batch of data `x` with target `y`, an optimizer, and a loss function. The function should return the loss value, as a float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQu7n7_itgW9"
      },
      "outputs": [],
      "source": [
        "def train_nn_step(\n",
        "    optimizer: optim.Optimizer,\n",
        "    loss_fn: nn.Module,\n",
        "    model: nn.Module,\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        ") -> float:\n",
        "    model.train()  # Used to ensure that relevant blocks are in training mode\n",
        "\n",
        "    # Your solution here #######################################################\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = loss_fn(model(x).squeeze(), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzRfczBgXGj"
      },
      "source": [
        "**1.3.4** Write an evaluation function that takes as input a PyTorch Module, a data loader, and a [TorchMetrics function][torchmetrics] and returns the cumulative metric over all batches.\n",
        "\n",
        "[TorchMetrics][torchmetrics] is a convenient package that implements metrics that work with PyTorch Tensors, and also with batched data.\n",
        "\n",
        "[torchmetrics]: https://lightning.ai/docs/torchmetrics/stable/pages/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P-metq4DA8E"
      },
      "outputs": [],
      "source": [
        "def eval_nn(model: nn.Module, loader: DataLoader, metric_fn: Metric) -> float:\n",
        "    model.eval()  # Used to ensure that relevant block are in evaluation model\n",
        "\n",
        "    # Your solution here #######################################################\n",
        "\n",
        "    metric_fn.reset()\n",
        "\n",
        "    for x, y_true in loader:\n",
        "        y_pred = model(x.to(device)).sigmoid().squeeze()\n",
        "        metric_fn(y_pred, y_true.to(device))\n",
        "\n",
        "    return metric_fn.compute().item()\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctd5U6RH0yHY"
      },
      "source": [
        "**1.3.5** Create an instance of the previously defined MLP, a relevant [loss function][torch_loss] for our classification task, and an optimizer.\n",
        "When needed, as for the MLP hidden dimension and optimizer learning rate, select parameters that provide good results for the task. You might need some trial-and-error, so keep track of you results as you will be asked to comment on those hyperparameters.\n",
        "\n",
        "Make sure to send everything to the correct device at initialization, as moving information from the CPU to the GPU is time-consuming.\n",
        "\n",
        "[torch_loss]: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "[torch_optim]: https://pytorch.org/docs/stable/optim.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H7iUiLHtp2K",
        "outputId": "0876d4ff-40bc-4742-cec2-4f183d2b360b"
      },
      "outputs": [],
      "source": [
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "mlp = MLP(data.x.shape[1], hidden_features=64).to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.005)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1gxH1xDBw0"
      },
      "source": [
        "**1.3.6** Perform 10 epochs of training. During each epoch, you should perform training steps iterating over the whole dataset. Gather the losses of each batch, and plot the evolution of the training loss at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 15\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "loss_list = []\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_losses = []\n",
        "    for x, y in loader_train:\n",
        "        loss = train_nn_step(\n",
        "            optimizer, loss_fn, mlp, x.to(device), y.to(device).double()\n",
        "        )\n",
        "\n",
        "        epoch_losses.append(loss)\n",
        "    \n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    loss_list.append(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180-fQrevQ64"
      },
      "source": [
        "**1.3.7** Evaluate the trained model on both the training and test data, using the most relevant metric from those already imported from TorchMetrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JETfZaKld3dl"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "metric_fn = BinaryF1Score().to(device)\n",
        "\n",
        "metric_tr = eval_nn(mlp, loader_train, metric_fn)\n",
        "metric_te = eval_nn(mlp, loader_test, metric_fn)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "print(f\"Training metric: {metric_tr:.3f}\")\n",
        "print(f\"Test metric:     {metric_te:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csB2WeikeKHD"
      },
      "source": [
        "**1.3.8** Try different hyperparameters' combinations, in particular for the hidden dimension of the MLP and the learning rate of the optimizer. Then discuss the obtained results and the learning curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX8v2A0OlhQ"
      },
      "source": [
        "**Your answer here:**\n",
        "Here is a table reporting the F1 score on test (train) data for some parameter combinations, in percentage.\n",
        "\n",
        "| LR \\ Hidden dim. | 64 | 128 | 256 |\n",
        "|-|-|-|-|\n",
        "| 0.01  | 90.1 (92.9) | 89.4 (93.5) | 89.6 (93.7) |\n",
        "| 0.005 | 90.1 (93.4) | 89.3 (94.1) | 88.7 (94.8) |\n",
        "| 0.001 | 90.7 (92.3) | 90.5 (93.2) | 90.3 (94.1) |\n",
        "\n",
        "With reduced LR and fixed hidden dimensions we see that the model takes longer to attain the same loss value; then it gets closer to local minima, which might either be a better or worse result.\n",
        "\n",
        "By increasing the hidden dimension, the model capacity increases, allowing us to get better results if we are lucky, but the chance to get stuck in a bad local minima increases.\n",
        "\n",
        "Overall, we are performing better than the classical ML methods!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJUsBUzO3pU"
      },
      "source": [
        "### Question 1.4: Graph Neural Networks\n",
        "\n",
        "We will now shift from the standard deep learning paradigm to Graph Neural Networks, to leverage the additional structure of our data.\n",
        "\n",
        "We already imported [PyTorch Geometric][torch_geometric] as `pyg`, so you can access its submodules as `pyg.nn`, `pyg.data`, and so on.\n",
        "\n",
        "[torch_geometric]: https://pytorch-geometric.readthedocs.io/en/latest/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0decSDJ6Cag"
      },
      "source": [
        "**1.4.1** Let's start by defining our first GNN. Again, it will be a subclass of the PyTorch `Module`, but this time it will take into account the `edge_index` in its `forward method`. Use two [GCN layers][gcn] to go from input features, here called *channels*, to a hidden dimension defined in the constructor, then to logit readout. Use ReLU activations.\n",
        "\n",
        "This GNN will map node vectors to node logits.\n",
        "\n",
        "[gcn]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhIU33lY6H-Q"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    # Your solution here #######################################################\n",
        "    def __init__(self, in_channels: int, hidden_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = pyg.nn.GCNConv(\n",
        "            in_channels=in_channels, out_channels=hidden_channels\n",
        "        )\n",
        "        self.conv2 = pyg.nn.GCNConv(in_channels=hidden_channels, out_channels=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwZEvTJlHbKb"
      },
      "source": [
        "**1.4.2** Perform `n_epochs` of training of a GCN model with 64 hidden channels, using full training data as a batch. Make sure to only use training data in the loss computation by using the `train_mask`. Track the loss value at each step and plot it. Finally, evaluate the model on train and test, using the `metric_fn` from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "pllLi-s9H79q",
        "outputId": "137a031d-2973-43a6-b63d-41f976b714da"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "def evaluate_gnn(model, data, train_mask, metric_fn):\n",
        "    y_pred = model(data).squeeze()\n",
        "\n",
        "    metric_fn.reset()\n",
        "    metric_tr = metric_fn(y_pred[train_mask], data.y[train_mask])\n",
        "\n",
        "    metric_fn.reset()\n",
        "    metric_te = metric_fn(y_pred[~train_mask], data.y[~train_mask])\n",
        "\n",
        "    return metric_tr, metric_te\n",
        "\n",
        "\n",
        "def train_gnn_full(model, data, loss_fn, optimizer, n_epochs):\n",
        "    loss_list = []\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(data).sigmoid().squeeze()\n",
        "        loss = loss_fn(y_pred[train_mask], data.y[train_mask].double())\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "gcn = GCN(data.x.shape[1], hidden_channels=64).to(device)\n",
        "optimizer = optim.Adam(gcn.parameters(), lr=0.01)\n",
        "\n",
        "loss_list = train_gnn_full(gcn, data, loss_fn, optimizer, n_epochs)\n",
        "\n",
        "metric_tr, metric_te = evaluate_gnn(gcn, data, train_mask, metric_fn)\n",
        "\n",
        "plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.show()\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "print(f\"Training metric: {metric_tr:.3f}\")\n",
        "print(f\"Test metric:     {metric_te:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceMxKgYzOmGI"
      },
      "source": [
        "**1.4.3** Hopefully, we got already some good results, but we would like to test whether stochastic optimization might be better. Batching graph data requires a particular approach, since on top of the design matrix with node features we have to account for edge information. In our setting, we have a single graph with many nodes, and a node level task. A batching strategy consists in sampling nodes with their neighbors, then working with this smaller graphs in a batched way.\n",
        "\n",
        "Define one [NeighborLoader][neighborloader] for the training data, which will gather neighbors for as many *iterations* as layers in your GCN. Check [mini batches](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches) if needed.\n",
        "\n",
        "[neighborloader]: https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbtiFv4a4aPW"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "loader_graph_train = pyg.loader.NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[-1] * 2,\n",
        "    input_nodes=train_mask,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g18kADvUSJV2"
      },
      "source": [
        "**1.4.4** Use the previously defined `train_nn_step` to train a newly initialized GCN with the new loader. Again plot the loss evolution and evaluate the trained model on train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "def train_gnn_batch(model, loader, loss_fn, optimizer, n_epochs):\n",
        "    epoch_loss_list = []\n",
        "    for epoch in range(n_epochs):\n",
        "        batch_losses = []\n",
        "        for batch in loader:\n",
        "            loss = train_nn_step(\n",
        "                optimizer, loss_fn, model, batch.to(device), batch.y.to(device).double()\n",
        "            )\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "        avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "        epoch_loss_list.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}: Average Loss = {avg_loss:.4f}\")\n",
        "    return epoch_loss_list\n",
        "\n",
        "gcn = GCN(data.x.shape[1], hidden_channels=64).to(device)\n",
        "optimizer = optim.Adam(gcn.parameters(), lr=0.001)\n",
        "\n",
        "loss_list = train_gnn_batch(gcn, loader_graph_train, loss_fn, optimizer, n_epochs)\n",
        "\n",
        "plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ymNskzZ2Mx"
      },
      "source": [
        "**1.4.5** Predict the label probabilities of each node and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCApzHuVd3ja"
      },
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "metric_tr, metric_te = evaluate_gnn(gcn, data, train_mask, metric_fn)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "print(f\"Training metric: {metric_tr:.3f}\")\n",
        "print(f\"Test metric:     {metric_te:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfw4YpuYbJwT"
      },
      "source": [
        "**1.4.6** Define a new GNN architecture using [graph attention layers][gat].\n",
        "\n",
        "[gat]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html#torch_geometric.nn.conv.GATv2Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgMuSdBgbmHP"
      },
      "outputs": [],
      "source": [
        "class GAT(nn.Module):\n",
        "    # Your solution here #######################################################\n",
        "    def __init__(self, in_channels: int, hidden_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = pyg.nn.GATv2Conv(\n",
        "            in_channels=in_channels, out_channels=hidden_channels\n",
        "        )\n",
        "        self.conv2 = pyg.nn.GATv2Conv(in_channels=hidden_channels, out_channels=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlTRMkPYb2zc"
      },
      "source": [
        "**1.4.7** Train and evaluate the GAT model with both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "mzVw3nEAX83K",
        "outputId": "607e7937-694d-4fce-f4cf-eecd8ff6ac11"
      },
      "outputs": [],
      "source": [
        "print(\"FULL TRAINING\")\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "gat = GAT(data.x.shape[1], hidden_channels=64).to(device)\n",
        "optimizer = optim.Adam(gat.parameters(), lr=0.05)\n",
        "\n",
        "loss_list = train_gnn_full(gat, data, loss_fn, optimizer, n_epochs)\n",
        "\n",
        "plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "\n",
        "metric_tr, metric_te = evaluate_gnn(gat, data, train_mask, metric_fn)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "plt.title(\"FULL TRAINING\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training metric: {metric_tr:.3f}\")\n",
        "print(f\"Test metric:     {metric_te:.3f}\")\n",
        "\n",
        "print(\"BATCH TRAINING\")\n",
        "\n",
        "# Your solution here ###########################################################\n",
        "\n",
        "gat = GAT(data.x.shape[1], hidden_channels=64).to(device)\n",
        "optimizer = optim.Adam(gat.parameters(), lr=0.01)\n",
        "\n",
        "loss_list = train_gnn_batch(gat, loader_graph_train, loss_fn, optimizer, n_epochs)\n",
        "\n",
        "plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "\n",
        "metric_tr, metric_te = evaluate_gnn(gat, data, train_mask, metric_fn)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "plt.title(\"BATCH TRAINING\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training metric: {metric_tr:.3f}\")\n",
        "print(f\"Test metric:     {metric_te:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWzbUPRwC_YE"
      },
      "source": [
        "## Section 2: Learning graphs\n",
        "\n",
        "Graph attention layers are quite interesting because they dynamically weight each neighbor based on both local and incoming information from a node. This mechanism is conceptually similar to learning an entirely new graph structure on top of the existing one.\n",
        "\n",
        "In this part, we design a block that, from node embeddings, will produce a new graph. The idea is similar to the one in the paper [Discrete Graph Structure Learning for Forecasting Multiple Time Series](https://openreview.net/forum?id=WEHSlH5mOk), which is illustrated in the following figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spfK3A4tUuP9"
      },
      "source": [
        "![Graph Learning Module](graph_learning_module.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8YFdReLEvP"
      },
      "source": [
        "As we can see from the schema, we have three main components:\n",
        "1. **Feature extractor**: Maps each node to a new, synthesized representation.\n",
        "2. **Link predictor**: for each pair of node representations, predicts the probability that an edge links them. We gather probabilities in a *structure matrix* $\\theta$.\n",
        "3. **Sampling**: Samples one, or multiple, discrete graphs from the structure matrix.\n",
        "\n",
        "In the following questions, we will break down these components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr50GNoAVK44"
      },
      "source": [
        "### Question 2.1: Sampling\n",
        "\n",
        "Sampling is the most intriguing part of our module, as it maps, randomly, continuous probabilities to discrete edges. Ideally, we would like to sample each edge with a probability $\\theta$, following a Bernoulli distribution, but this would be hard to backpropagate through.\n",
        "\n",
        "What we do instead is known as the **Gumbel Trick**.\n",
        "We sample edges using a [Gumbel][gumbel] reparameterization, which allows differentiating for $\\theta$ through it. With $g_{ij}^1, g_{ij}^2 \\sim \\operatorname{Gumbel}(0,1)$ for all $i,j$, and $s$ a temperature parameter,\n",
        "$$\n",
        "A_{ij} = \\operatorname{sigmoid}\\left(\n",
        "  \\frac{\n",
        "    \\log\\left( \\frac{\\theta_{ij}}{1 - \\theta_{ij}} \\right)\n",
        "    + g_{ij}^1 - g_{ij}^2\n",
        "  }{s}\n",
        "\\right)\n",
        ".\n",
        "$$\n",
        "By letting the temperature go to zero, we can get closer and closer to a Bernoulli distribution.\n",
        "\n",
        "[gumbel]: https://en.wikipedia.org/wiki/Gumbel_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmZ4U5FBTylP"
      },
      "source": [
        "**2.1.1** Define a function to sample a matrix of Gumbel variables of given shape, knowing that, for $p$ sampled uniformly in (0,1), then $Q(p) \\sim \\operatorname{Gumbel}(\\mu,\\beta)$\n",
        "$$\n",
        "  Q(p)=\\mu-\\beta \\ln (-\\ln (p))\n",
        "  .\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMPBfsrvcFA7",
        "outputId": "fdce3b7b-8031-41f5-e828-35571c3186cb"
      },
      "outputs": [],
      "source": [
        "def sample_gumbel(shape, mu=0, beta=1):\n",
        "    # Your solution here #######################################################\n",
        "\n",
        "    U = torch.rand(shape)\n",
        "    return mu - beta * torch.log(-torch.log(U))\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "print(\"Testing sample_gumbel\")\n",
        "print(sample_gumbel((2, 3), 0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1c-H65hZjIj"
      },
      "source": [
        "**2.1.2** Note that $log(\\frac{\\theta}{1 - \\theta})$ is the sigmoid function, so we can work with unnormalized edge logits instead of probabilities. Define a function to sample an adjacency matrix $A$ from edge logits using the Gumbel Trick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoQk5QzOXlLM",
        "outputId": "af7347b7-5844-4edf-daa7-78d8fe71c4e1"
      },
      "outputs": [],
      "source": [
        "def sample_gumbel_trick(logits, temperature, mu=0, beta=1):\n",
        "    # Your solution here #######################################################\n",
        "    g1 = sample_gumbel(logits.shape)\n",
        "    g2 = sample_gumbel(logits.shape)\n",
        "\n",
        "    return torch.sigmoid((logits + g1 - g2) / temperature)\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "print(\"Testing sample_gumbel_trick\")\n",
        "print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=10))\n",
        "print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=1))\n",
        "print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=1e-2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umLVURybMrBn"
      },
      "source": [
        "### Question 2.2: Link predictor\n",
        "\n",
        "GNNs are all about node embeddings, which by now we should know how to deal with. The missing component is therefore the **link predictor**. Naively, we could iterate through all pairs of nodes and apply a predictor layer, but it would be highly inefficient. To leverage tensor manipulation, let's start by gathering paired node representations in a matrix, so that we can predict probabilities in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Y-vdSGONQO"
      },
      "source": [
        "**2.2.1**  Define a function that takes as input a tensor of node embeddings, and returns a tensor that concatenates embeddings pairwise. Use [triu_indices][triu_indices] to have pairs appearing only once and avoid self loops, and return the indices along with the embeddings.\n",
        "\n",
        "[triu_indices]: https://pytorch.org/docs/stable/generated/torch.triu_indices.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLUKHp5jlXbs",
        "outputId": "6c17e24f-1a26-4f26-c098-8fcd5be9f967"
      },
      "outputs": [],
      "source": [
        "def pair_embeddings(x) -> (torch.Tensor, torch.Tensor):\n",
        "    # Your solution here ###########################################################\n",
        "    triu = torch.triu_indices(x.shape[-2], x.shape[-2], offset=1)\n",
        "    return torch.cat([x[..., triu[0], :], x[..., triu[1], :]], dim=-1), triu\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "print(\"Testing pair_embeddings\")\n",
        "print(pair_embeddings(torch.tensor([[1.0], [2.0], [3.0]]))[0])\n",
        "print(pair_embeddings(torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]]))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoAjpxhcc6No"
      },
      "source": [
        "**2.2.2** Define a PyTorch module that takes as input node embeddings, compute link probabilities with a two-layer perceptron on paired embeddings, then samples edges with the Gumbel trick. The output of the forward method will be a PyTorch Geometric [EdgeIndex][edge_index] of tensors representing indices and weights corresponding to positively sampled edges. You might need a `eps` threshold to avoid numerical errors.\n",
        "\n",
        "[edge_index]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.EdgeIndex.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "IH4IFTq2dSex",
        "outputId": "5ec53484-b2e8-4e04-857b-aea8ff2ff52a"
      },
      "outputs": [],
      "source": [
        "class MLPGraphLearn(nn.Module):\n",
        "    # Your solution here ###########################################################\n",
        "    def __init__(\n",
        "        self, in_features: int, hidden_features: int, temperature: float, eps=1e-10\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.link_predictor = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=2 * in_features,\n",
        "                out_features=hidden_features,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(\n",
        "                in_features=hidden_features,\n",
        "                out_features=1,\n",
        "                bias=False,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.temperature = temperature\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_nodes = x.shape[-2]\n",
        "        x, triu = pair_embeddings(x)\n",
        "\n",
        "        logits = self.link_predictor(x)\n",
        "\n",
        "        weights = sample_gumbel_trick(logits, self.temperature).squeeze()\n",
        "\n",
        "        edge_index = torch.cat([triu, triu.flip(-1)], dim=-1)\n",
        "        weights = torch.cat([weights, weights])\n",
        "\n",
        "        return pyg.EdgeIndex(\n",
        "            edge_index,\n",
        "            sparse_size=(n_nodes, n_nodes),\n",
        "            sort_order=\"row\",\n",
        "            is_undirected=True,\n",
        "        )[:, weights > self.eps]\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "print(\"Testing MLPGraphLearn\")\n",
        "mlp_gl = MLPGraphLearn(2, 5, 0.01)\n",
        "mlp_gl(torch.tensor([[[1.0, 2.0], [0.5, 7.1], [-0.1, 0.3]]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yTN8dYkign"
      },
      "source": [
        "### Question 2.3: Classifiers\n",
        "\n",
        "Let's introduce our graph learning block into some classifiers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR_4B2aCk7ZI"
      },
      "source": [
        "**2.3.1** Define a classifier that first produces node embeddings with a linear layer with ReLU activation, which it feeds to the previously defined GL module, then it performs two graph convolutions on the original node features using the learned graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iix9QMkXlNk"
      },
      "outputs": [],
      "source": [
        "class MLPGLClassifier(nn.Module):\n",
        "    # Your solution here #######################################################\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        gl_node_features_in: int = 64,\n",
        "        gl_node_features_hidden: int = 32,\n",
        "        gcn_hidden: int = 64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.feat_extractor = nn.Linear(\n",
        "            in_features=in_features,\n",
        "            out_features=gl_node_features_in,\n",
        "        )\n",
        "        self.graph_learn = MLPGraphLearn(\n",
        "            in_features=gl_node_features_in,\n",
        "            hidden_features=gl_node_features_hidden,\n",
        "            temperature=1,\n",
        "        )\n",
        "\n",
        "        self.conv1 = pyg.nn.GCNConv(in_channels=in_features, out_channels=gcn_hidden)\n",
        "        self.conv2 = pyg.nn.GCNConv(in_channels=gcn_hidden, out_channels=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        edge_index = self.graph_learn(self.feat_extractor(x).relu())\n",
        "\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9mr7BGmqZ15"
      },
      "source": [
        "**2.3.2** Unfortunately, training and evaluating the `MLPGLClassifier` might take too long. Let's just test whether it works: instantiate the classifier and compute the graph embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your solution here ###########################################################\n",
        "\n",
        "gl_classifier = MLPGLClassifier(data.x.shape[1]).to(device)\n",
        "gl_classifier(data)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
