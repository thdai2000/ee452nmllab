{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NML-25] Notebook 1: Introduction to the python toolbox\n",
    "\n",
    "Responsible TA: [Jeremy Baffou](https://people.epfl.ch/jeremy.baffou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "Troughout the different lab session, you will have coding and theoretical questions. Coding exercises shall be solved within the specified space:\n",
    "```python\n",
    "# Your solution here ###########################################################\n",
    "...\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "```\n",
    "Sometimes we provide variable names, such as `x = ...`; do not change names and stick to hinted typing, as they will be reused later.\n",
    "Within the solution space, you can declare any other variable of function that you might need, but anything outside these lines shall not be changed, or it will invalidate your answers.\n",
    "\n",
    "Theoretical questions shall be answered in the following markdown cell. The first line will be \n",
    "```markdown\n",
    "**Your answer here:**\n",
    "...\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "* Your code should be self-contained in the `.ipynb` file. The solution to the exercices will be provided in an external `.ipynb` file.\n",
    "\n",
    "* Try to make your code clean and readable, it is a good training for the project. Provide meaningful variable names and comment where needed.\n",
    "\n",
    "* You cannot import any other library than we imported, unless explicitly stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This goal of this notebook is to have an introduction (or refresh) to elementary Python libaries and graph related toolbox. The exercices can be divided in two sections:\n",
    "* Elementary Python Toolbox\n",
    "* Basics of graph modelling/analysis\n",
    "\n",
    "The first part covers some basic Python libraries that will be used multiple times during the exercice sessions and the project, namely [Pandas](https://pandas.pydata.org/docs/), [NumPy](https://numpy.org/devdocs/user/index.html) and [Scikit-learn](https://scikit-learn.org/stable/user_guide.html). If you know those libraries, most task in this section will be straightforward. If you struggle with some aspects, take time to revise the library documentations, as next sessions will rely on these tools. \n",
    "\n",
    "The second part of the notebook will introduce [Networkx](https://networkx.org/documentation/latest/tutorial.html), a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. We will start by building graphs from edge lists and from features, and then we will explore some basic graph properties. A tutorial on networkx can be found [here](https://networkx.github.io/documentation/stable/tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I: Introduction to elementary Python toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Palmer Archipelago (Antarctica) penguin data](https://github.com/allisonhorst/palmerpenguins/tree/main) for this exercise session.\n",
    "\n",
    "We provide a simplified version of the data in `penguins_size.csv`\n",
    "\n",
    "Dataset reference: https://doi.org/10.5281/zenodo.3960218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pandas, to manipulate tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Data loading and examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1** Read the `penguins_size.csv` file into a Pandas DataFrame, using the `read_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "penguins: pd.DataFrame = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2** Extract the first five rows of the data frame and 10 random ones, then concatenate and display them. You can use the built-in `display` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "display(...)\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3** Look at the fourth entry: it is missing information, which is filled with `NaN` (not a number) values.\n",
    "Let's drop all rows with missing values, then display the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4** Compute and display the mean and std of `culmen_length_mm` and `body_mass_g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "print(\"Mean values:\")\n",
    "...\n",
    "\n",
    "print(\"Standard deviation:\")\n",
    "...\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.5** Examine statistics of all columns with the `describe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.6** Plot a histogram of `body_mass_g`, split by `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Note that one penguin seems to have a missing value for the sex entry, leading to the additionnal pannel in the histogram.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.7** The [seaborn](https://seaborn.pydata.org/tutorial.html) library provides nicer visualization functionalities. Let's produce the same histogram with it. (Hint: Try to use the `hue` argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: indexing and manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy allows manipulating vectors, matrices and higher order tensors as `arrays`. For instance vectors are 1d arrays, and matrices have two dimensions (rows and columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.1** Use the `loc` property to remove penguins without sex. (The `value_counts()` function should return only `Male` and `Female` entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "penguins[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2** Make `sex` a boolean property, with value `True` for `\"FEMALE\"` and `False` for `\"MALE\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "penguins[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3** In the next questions we will encode numerically the `island` and `species` property. Let's start by identifying the set of unique island's and species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "islands: list[str] = ...\n",
    "species: list[str] = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"islands:\", islands)\n",
    "print(\"species:\", species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.4** For each island and species, add a column with boolean value, indicating whether the penguin comes from said island, or belong to said species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "display(penguins.loc[[13, 26, 39, 52]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.5** In some case, we might want to encode species as integers.\n",
    "Use the `map` method and a dictionary mapping species to numbers to get a `y_species` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "y_species: pd.Series = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "y_species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.6** Drop the `island` and `species` columns, since they are not needed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "assert 'species' not in penguins.columns and 'island' not in penguins.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: NumPy, scientific computing in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Array Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1** Pandas is built on top of NumPy. We can access the underlying array though the `values` attribute of a DataFrame. Let's put all but the `body_mass_g` columns in the *design matrix* `x`, and the y_mass one in the *target vector* `y_mass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "x: np.ndarray = ...\n",
    "y_mass: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2** Let's inspect the `shape` of these two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "print(\"x shape:\", ...)\n",
    "print(\"y shape:\", ...)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.3** Then let's check the first five rows of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.4** Notice that the `dtype` of `x` is `object`, which means that it contains multiple types (namely `float` and `bool`). Let's convert it to `float` and check the first rows again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Array manipulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.1** Extract the values of `Dream` and `Gentoo` columns into two vectors. Convert them to boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream: np.ndarray\n",
    "gentoo: np.ndarray\n",
    "# Your solution here ###########################################################\n",
    "dream, gentoo = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2** Count how many penguins come from the Dream island using the `sum` method. Repeat for the Gentoo specie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "print(\"Dream's penguins:\", ...)\n",
    "print(\"Gentoo's penguins:\", ...)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.3** You can use a boolean mask to extract values from an array.\n",
    "Compute the average mass and std of Dream's penguins using the corresponding NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "print(\"Average Dream's mass:\", ...)\n",
    "print(\"Dream's mass std:\", ...)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.4** Now, compute again the average y_mass of Dream's penguins but without the `mean`function. Try using the scalar product between the mass vector and the Dream boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "dream_avg_mass: float = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"Average Dream's mass:\", dream_avg_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.5** Compute the standard deviation as an inner product too and compare with the values from previous answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "dream_std_mass: float = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"Dream's mass std:\", dream_std_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression aims to find a weight vector $\\mathbf w$ such that the target value $y_i$ can be retrieved as a weighted sum of the corresponding features $\\mathbf x_i$, or in matrix notation\n",
    "$$ \\mathbf{X w} = \\mathbf y. $$\n",
    "\n",
    "Most of the time we cannot find an exact solution to this problem, therefore we introduce an error function and look for weights that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1** Find a solution for `w` by solving a linear system with `np.linalg.solve`.\n",
    "For this method to work you need as many equations as variables, thus as many samples as the number of variables. Choose them randomly with `np.random.choice`.\n",
    "\n",
    "Note that by selecting random rows, your design matrix might become singular. You can use `try` to rerun until it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = x.shape\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Your solution here ###################################################\n",
    "\n",
    "        w_solve: np.ndarray = ...\n",
    "\n",
    "        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        break\n",
    "    except np.linalg.LinAlgError:\n",
    "        pass\n",
    "\n",
    "print(\"Weights:\", w_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.2** Define a function to compute the mean squared error ([MSE](https://en.wikipedia.org/wiki/Mean_squared_error)) between the real `y` and the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: np.array, y_pred: np.array) -> float:\n",
    "    # Your solution here #######################################################\n",
    "    return ...\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.3** Use the previously computed weights to predict the penguins masses, then compute their MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "mse_solve: float = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"MSE of random subproblem:\", mse_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.4** Randomly selecting samples is suboptimal as we ignore a significant part of the dataset. Let's look for a solution that uses all the data by using the pseudoinverse of `x`. (Hint: Remember the Linear Regression Equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "w_pinv: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.5** Compute the MSE of this solution prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "mse_pinv: float = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"MSE of psudoinverse solution:\", mse_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we focus on a convenient way to manipulate arrays which allows parallelizing operations over the same input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1** extract all island one-hot encoding from the data frame and convert to boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "islands_oh: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"islands shape\", islands_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.2** Multiply the penguin masses to `islands_oh` to mask them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Your solution here ###########################################################\n",
    "    masked_mass: np.ndarray = ...\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "except ValueError as err:\n",
    "    print(\"There's an ERROR:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.3** The error indicates that arrays of different shapes cannot be automatically broadcasted! Using `np.newaxis`, add a dimension to `y_mass` and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "masked_mass: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.4** Compute the average masses over different islands summing the masked masses along the corresponding axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "avg_masses: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"Average masses:\")\n",
    "print(dict(zip(islands, avg_masses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.5** Use broadcasting and masking to compute standard deviations for each island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "std_masses: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"Mass standard deviations:\")\n",
    "print(dict(zip(islands, std_masses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: K-Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a k-nearest neighbors (kNN) classifier to identify penguin species from their physical attributes.\n",
    "\n",
    "For a query data point, kNN predicts its label as the most frequent one between those of the k closest samples from the training dataset. In this setting, we will use euclidean distance between points:\n",
    "$$ d(\\mathbf x_i, \\mathbf x_j) = \\sqrt{\\sum_{n=1}^D (x_{id} - x_{jd})^2} $$\n",
    "\n",
    "In the next cell, we prepare to split the data in training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_attributes = [\n",
    "    \"culmen_length_mm\",\n",
    "    \"culmen_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "]\n",
    "\n",
    "# We count the number of samples and set the amount of training data to 70% of that\n",
    "n_samples = len(penguins)\n",
    "samples_tr = int(n_samples * 0.7)\n",
    "\n",
    "# We shuffle the indices of the data and take the first 70% to be in training\n",
    "# Working with indices allows us to recognise which features go with which labels\n",
    "rng = np.random.default_rng(11)\n",
    "shuffled = rng.permutation(np.arange(n_samples))\n",
    "idx_tr = shuffled[:samples_tr]\n",
    "idx_te = shuffled[samples_tr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.1** Extract training and test features from the `penguins` data frame using the indices defined above and split the `y_species` series (computed in 1.2.5) accordingly.\n",
    "\n",
    "You can use `DataFrame.iloc` to work with integer indexing in pandas.\n",
    "Remember to extract arrays from data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "x_tr: np.ndarray = ...\n",
    "x_te: np.ndarray = ...\n",
    "y_tr: np.ndarray = ...\n",
    "y_te: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "print(\"Training features:\", x_tr.shape)\n",
    "print(\"Test features:\", x_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.2** Write a function that computes pairwise distances between all query points and training ones. Use broadcasting and sum along corresponding axes to get an efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(x_query: np.ndarray, x_tr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute pairwise distances\n",
    "\n",
    "    Args:\n",
    "        x_query (np.ndarray): Array of shape (n_queries, n_features)\n",
    "        x_tr (np.ndarray): Array of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Distances in array of shape (n_queries, n_samples)\n",
    "    \"\"\"\n",
    "    # Your solution here #######################################################\n",
    "    return ...\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "pdists_te = pairwise_distances(x_te, x_tr)\n",
    "print(\"Distances shape:\", pdists_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.3** For each query, identify the closest training samples using `np.argsort`. Use the `axis` argument to avoid iterating over the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "nearest_ngbs_te: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "print(\"5 nearest neighbors for query 4:\", nearest_ngbs_te[4, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.4** Write a function that takes an array of nearest neighbors for each query, together with training labels and the predefined $k$ and returns the predicted labels for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(nearest_ngbs: np.ndarray, y_tr: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Predict labels from k-nearest neighbors\n",
    "\n",
    "    Args:\n",
    "        nearest_ngbs (np.ndarray): Array of neighbors indices, sorted by distance.\n",
    "            Shape: (n_queries, n_samples)\n",
    "        y_tr (np.ndarray): Training labels of shape (n_samples,)\n",
    "        k (int): number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels of shape (n_queries,)\n",
    "    \"\"\"\n",
    "    # Your solution here #######################################################\n",
    "    # Extract nearest ngbs labels\n",
    "    ...\n",
    "\n",
    "    # Count label occurrencies for each query\n",
    "    ...\n",
    "\n",
    "    # Return most frequent occurrence\n",
    "    return ...\n",
    "\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Predicted labels of the first 10 queries:\",\n",
    "    predict_labels(nearest_ngbs_te[:10], y_tr, 5),\n",
    ")\n",
    "print(\"Real labels:                             \", y_te[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5.5** Compute the precision of kNN's predictions for both the training and test datasets for all k between 1 and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "precisions_te: list[float] = ...\n",
    "\n",
    "precisions_tr: list[float] = ...\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 3), dpi=100)\n",
    "ax.plot(precisions_tr, label=\"Train\")\n",
    "ax.plot(precisions_te, label=\"Test\")\n",
    "ax.set(title=\"My kNN implmentation\", ylabel=\"precision\", xlabel=\"k value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scikit-learn, machine learning toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides implementations for many machine learning algorithms, which all share the basic common interface of `fit` and `predict` methods. Let's compare it to our kNN implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Import the k nearest neighbor classifier from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "from sklearn ...\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Import the precision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Compute train and test precision for all k between 1 and 30. Use the \"micro\" option for the precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "sk_prec_tr: list[float] = ...\n",
    "sk_prec_te: list[float] = ...\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Plot scores and compare to your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 3), dpi=100)\n",
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "ax.set(title=\"SkLearn kNN implementation\", ylabel=\"precision\", xlabel=\"k value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II: Introduction to Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line is a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html). It enables plotting inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also try `%matplotlib notebook` for a zoomable version of plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building Graphs from Edge Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will play with a partial segment of the Tree of Life. The full version is available here: [Open Tree of Life](https://tree.opentreeoflife.org/about/taxonomy-version/ott3.0). In this tutorial, the dataset is reduced to the first 999 taxons (starting from the root node), which can be found in `data/taxonomy_small.tsv`.\n",
    "\n",
    "![Public domain, https://en.wikipedia.org/wiki/File:Phylogenetic_tree.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Phylogenetic_tree.svg/800px-Phylogenetic_tree.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, change this variable to the relative path to the taxonomy file\n",
    "path ='./data/taxonomy_small.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life = pd.read_csv(path, sep='\\t\\|\\t?', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick recap, we will go through a guided preliminary exploration of the dataset. We will use what we learned about `pandas` in Section 1.\n",
    "\n",
    "We start by looking at the head of the dataframe.\n",
    "The description of the entries is given here:\n",
    "https://github.com/OpenTreeOfLife/reference-taxonomy/wiki/Interim-taxonomy-file-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let us now drop some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life = tree_of_life.drop(columns=['sourceinfo', 'uniqname', 'flags','Unnamed: 7'])\n",
    "tree_of_life.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pandas infers the type of values inside each column (in this case: int, float, string and string - run below).\n",
    "The parent_uid column has float values because there was a missing value, converted to `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Types of the columns in the dataframe:\")\n",
    "for col in tree_of_life.columns:\n",
    "    print(f\"{col}: {tree_of_life[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To order the data, we can use the function `sort_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life.sort_values(by='name').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Remark:* Some functions do not change the dataframe (option `inline=False` by default). As you can see below, the `tree_of_life` dataframe remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classes of `rank` do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life['rank'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we filter only `species` entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life[tree_of_life['rank'] == 'species'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let us now find how many species entries do we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tree_of_life[tree_of_life['rank'] == 'species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the possible `rank`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_of_life['rank'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Operations on columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1** Display the entry with name 'Archaea'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2** Display the entry of its parent. _(Hint: Remember of how the parent_id is used to link entities in the tree)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting relevant information for the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the adjacency matrix of the graph. For that we need to reorganize the data. First we separate the nodes and their properties from the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = tree_of_life[['uid', 'name','rank']]\n",
    "edges = tree_of_life[['uid', 'parent_uid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using an adjacency matrix, nodes are indexed by their row or column number and not by a `uid`. Let us create a new index for the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for node index.\n",
    "nodes.reset_index(level=0, inplace=True)\n",
    "nodes = nodes.rename(columns={'index':'node_idx'})\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversion table from uid to node index.\n",
    "uid2idx = nodes[['node_idx', 'uid']]\n",
    "uid2idx = uid2idx.set_index('uid')\n",
    "uid2idx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the edges, we should leverage a powerful function of `pandas`: the `join` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column, matching the uid with the node_idx.\n",
    "edges = edges.join(uid2idx, on='uid')\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with the parent_uid.\n",
    "edges = edges.join(uid2idx, on='parent_uid', rsuffix='_parent')\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the uids.\n",
    "edges_renumbered = edges.drop(columns=['uid','parent_uid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `edges_renumbered` table is a list of renumbered edges connecting each node to its parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_renumbered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the (unweighted and undirected) adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3** We will use numpy to build this matrix. Note that we don't have edge weights here, so our graph is going to be unweighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = len(nodes)\n",
    "adjacency = np.zeros((n_nodes, n_nodes), dtype=int)\n",
    "\n",
    "for idx, row in edges.iterrows():\n",
    "  # Your solution here #########################################################\n",
    "  pass\n",
    "  #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "adjacency[:15, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have built the adjacency matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a graph object from the adjacency matrix using networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple command to create the graph from the adjacency matrix.\n",
    "graph = nx.from_numpy_array(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let us add some attributes to the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_props = nodes.to_dict()\n",
    "\n",
    "for key in node_props:\n",
    "    nx.set_node_attributes(graph, node_props[key], key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if it is correctly stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.nodes[3])\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Alternative ways to build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.1** Build the graph directly from the `edges` table (without using the adjacency matrix). _(Hint: Create an empty graph object and iteratively add the edges)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert len(graph.nodes) == len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2** Build the graph from the initial `tree_of_life` table by directly iterating over the rows of this table (without building the adjacency matrix).\n",
    "\n",
    "Store the final graph in the variable `graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3** Get the adjacency matrix with `nx.adjacency_matrix(graph)` and compare it with what we obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, let us visualize the graph. We will again take advantage of the readily available functions in `networkx`.\n",
    "\n",
    "Draw the graph with two different [layout algorithms](https://en.wikipedia.org/wiki/Graph_drawing#Layout_methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spectral(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the graph to disk in the `gexf` format, readable by gephi and other tools that manipulate graphs. You may now explore the graph using [gephi](https://gephi.org/) and compare the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your saving path\n",
    "savepath = './tree_of_life.gexf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(graph, savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Graphs from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It often happens in real life that, when we wish to represent data as a graph, we do not have access to the underlying ground truth network. In this case, we need to make some assumptions about the graph/network structure. We will now explore a very basic and intuitive approach for graph construction based on similarity between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will play with the famous Iris dataset. This dataset can be found in many places on the net and was first released at <https://archive.ics.uci.edu/ml/index.php>. For example it is stored on [Kaggle](https://www.kaggle.com/uciml/iris/), with many demos and Jupyter notebooks you can test (have a look at the \"kernels\" tab).\n",
    "\n",
    "![Iris Par Za — Travail personnel, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=144395](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Iris_germanica_002.jpg/251px-Iris_germanica_002.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(path)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the entries is given here:\n",
    "https://www.kaggle.com/uciml/iris/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate statistics\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a graph from the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a graph from these data. The idea is to represent iris samples (rows of the table) as nodes, with connections depending on their physical similarity.\n",
    "\n",
    "The main question is how to define the notion of similarity between the flowers. For that, we need to introduce a measure of similarity. It should use the properties of the flowers and provide a positive real value for each pair of samples.\n",
    "\n",
    "*Remark:* The value should increase with the similarity.\n",
    "\n",
    "Let us separate the data into two parts: physical properties (`features`) and labels (`species`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.loc[:, ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "species = iris.loc[:, 'Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity, distance and edge weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define many similarity measures. One of the most intuitive and perhaps the easiest to program relies on the notion of distance. If a distance between samples is defined, we can compute the weight accordingly: if the distance is small, which means the nodes are similar, we want a strong connection between them (large weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine distance is a good candidate for high-dimensional data. It is defined as follows:\n",
    "$$d(u,v) = 1 - \\frac{u \\cdot v} {\\|u\\|_2 \\|v\\|_2},$$\n",
    "where $u$ and $v$ are two feature vectors.\n",
    "\n",
    "The distance is proportional to the angle formed by the two vectors (0 if colinear, 1 if orthogonal, 2 if opposed direction).\n",
    "\n",
    "Alternatives are the [$p$-norms](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#p-norm) (or $\\ell_p$-norms), defined as\n",
    "$$d(u,v) = \\|u - v\\|_p,$$\n",
    "of which the Euclidean distance is a special case, with $p=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Compute the Euclidean pairwise distances of all the points in the data. Use the `pdist` function from `scipy` (already imported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "distances: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# other optional metrics: 'cosine', 'cityblock', 'minkowski'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a distance, we can compute the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common function used to turn distances into edge weights is the Gaussian function:\n",
    "$$\\mathbf{W}(u,v) = \\exp \\left( \\frac{-d^2(u, v)}{\\sigma^2} \\right),$$\n",
    "where $\\sigma$ is the parameter which controls the width of the Gaussian.\n",
    "  \n",
    "The function giving the weights should be positive and monotonically decreasing with respect to the distance. It should take its maximum value when the distance is zero, and tend to zero when the distance increases. Note that distances are non-negative by definition. So any funtion $f : \\mathbb{R}^+ \\rightarrow [0,C]$ that verifies $f(0)=C$ and $\\lim_{x \\rightarrow +\\infty}f(x)=0$ and is *strictly* decreasing should be adapted. The choice of the function depends on the data.\n",
    "\n",
    "Some examples:\n",
    "* A simple linear function $\\mathbf{W}(u,v) = \\frac{d_{max} - d(u, v)}{d_{max} - d_{min}}$. As the cosine distance is bounded by $[0,2]$, a suitable linear function for it would be $\\mathbf{W}(u,v) = 1 - d(u,v)/2$.\n",
    "* A triangular kernel: a straight line between the points $(0,1)$ and $(t_0,0)$, and equals to 0 beyond it.\n",
    "* The logistic kernel $\\left(e^{d(u,v)} + 2 + e^{-d(u,v)} \\right)^{-1}$.\n",
    "* An inverse function $(\\epsilon+d(u,v))^{-n}$, with $n \\in \\mathbb{N}^{+*}$ and $\\epsilon \\in \\mathbb{R}^+$.\n",
    "* You can find some more [here](https://en.wikipedia.org/wiki/Kernel_%28statistics%29).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Let's use the Gaussian function. Define the gaussian's width (Hint: think what would make sense to define as $\\sigma^2$ considering the statistics of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "weights_list: np.ndarray = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of weights into a matrix.\n",
    "weight_matrix = squareform(weights_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Find the nodes with highest degree and display their respective entry in the `iris` dataframe. Do they belong to the same iris species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained a full matrix but we may not need all the connections (reducing the number of connections saves some space and computations!). We can sparsify the graph by removing the values (edges) below some fixed threshold. Let's see what kind of threshold we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weights_list)\n",
    "plt.title('Distribution of weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4:** Plot the number of edges with respect to the threshold, for threshold values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark:* The distances presented here do not work well for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5:** Based on the above plot define a threshold for the sparsification. Remember, too high might result in disconnected components. Too low might result in a graph that is still too dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "thresh: float = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "weight_matrix[weight_matrix < thresh] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, let us visualize the graph. We will use the python module networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple command to create the graph from the adjacency matrix.\n",
    "graph = nx.from_numpy_array(weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try some direct visualizations using networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us add some colors\n",
    "colors = species.values\n",
    "colors[colors == 'Iris-setosa'] = 0\n",
    "colors[colors == 'Iris-versicolor'] = 1\n",
    "colors[colors == 'Iris-virginica'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spectral(graph, node_color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be separated in 3 parts! Are they related to the 3 different species of iris?\n",
    "\n",
    "Let's try another [layout algorithm](https://en.wikipedia.org/wiki/Graph_drawing#Layout_methods), where the edges are modeled as springs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graph, node_color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the graph to disk in the `gexf` format, readable by gephi and other tools that manipulate graphs. You may now explore the graph using gephi and compare the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = './'\n",
    "nx.write_gexf(graph, os.path.join(savepath,'iris.gexf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6:** Modify the experiment such that the distance is computed using normalized features, i.e., all features (columns of `features`) having the same mean and variance (hint: use pandas to compute the statistics of the dataframe's column).\n",
    "This avoids having some features with too much importance in the computation of distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "features_norm = ...\n",
    "\n",
    "# The rest is the same, define the edges with a kernel function, sparsify the edges with thresholding and build the graph. Visualize the graph and compare it with the one built with the original feature values.\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Graph Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look into some tools we can use to derive interesting properties of the graph under study. More precisely, we will be interested in the notion of [**Centrality**](https://en.wikipedia.org/wiki/Centrality) and [**Clustering**](https://en.wikipedia.org/wiki/Clustering_coefficient).\n",
    "\n",
    "Briefly, let's recall that:\n",
    "- Centrality refers to how much a given node is important in a network. The definition of importance depends on the application, it can be structural or related to features.\n",
    "- Clustering is a measure of how often nodes tends to form cluster.\n",
    "\n",
    "To perform such analysis, let's travel a bit and cross the ocean, direction New York!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset curated thanks to https://github.com/gboeing/osmnx\n",
    "# Can be downloaded here: https://www.kaggle.com/datasets/crailtap/street-network-of-new-york-in-graphml?resource=download\n",
    "ny_graph = nx.read_graphml('./data/manhatten.graphml.xml')\n",
    "print(type(ny_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the network of the different street of Manhattan, with nodes being cross-point and edges being roads. We can see that this graph has a specific type: `MultiDiGraph`. This means it is a directed multi-graph (two nodes can have multiple edges with different properties between them). In real life, it may occurs when we try to encode different type of relationships between different entities. However, in our setup we are interested simply in the road and not in extra possible link it may exist between. This is a bit too much information for now, so let's focus on key properties of the graph and convert it to a simpler representation: A directed graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Modelling and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Create a DiGraph (directed graph) from the `ny_graph`. _(Hint: Create an directed empty graph object and iteratively add the edges by keeping only the first one encountered if multiple exist.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "simpler_ny_graph = ...\n",
    "for u, v, data in ny_graph.edges(data=True):\n",
    "    ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(simpler_ny_graph) == nx.DiGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we have our directed graph. However, depending on our construction, we may have lose node features. For now on we will only be interested in the spatial coordinates (x,y) of the different cross points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Retrieve the x,y coordinates of each node in the initial graph `ny_graph` and store them in the simpler one `simpler_ny_graph`. _(Hint: Look at nx.get_node_attributes() and nx.set_node_attributes(). The name of the features can be accessed by indexing the graph with a node id.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "x_pos : dict = ...\n",
    "y_pos : dict = ...\n",
    "... # store x,y coordinates in `simpler_ny_graph`\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced the graph as being a map on Manhattan, it is now time to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw_networkx(simpler_ny_graph,arrows=False,ax=ax,node_size=5,with_labels=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would probably agree that it is not the clearest map of new york ever created. This is due to how networkx plots graph. If you do not provide with insights on where the nodes are located, it will try to make a \"meaningful\" representation of the graph. But it is not always a success... Hopefully we do have spatial information about the nodes! What a chance we kept those x and y coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Using the `x_pos` and `y_pos`, create a dictionnary that maps each node id to a numpy array with the coordinates in float format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "pos = dict()\n",
    "for node_id in simpler_ny_graph.nodes:\n",
    "    pos[node_id] = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pos['2895441122'].dtype == np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot again our graph. Note how it is much faster because it doesn't have to compute a complicated layout for the graph now that it has the node coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw_networkx(simpler_ny_graph,pos,arrows=False,ax=ax,node_size=5,with_labels=False)\n",
    "fig.suptitle('View of Manhattan Roads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph modelling the underlying network of the street of New York can be studied in many ways. We will first focus on the notion of [clustering](https://en.wikipedia.org/wiki/Clustering_coefficient). Recall from the lecture that the clustering coefficient of a node quantifies how close to a clique (fully connected graph) is the subgraph defined by the node and its neighborhood.\n",
    "\n",
    "Before doing the computations, based on the very specific structure of the street of new york, do you think that the clustering coefficient will be on average low or high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Compute the clustering coefficients for all nodes in the graph. _(Starting from this exercice and for the rest of the notebook, only use `simpler_ny_graph` for computations)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "ny_clustering : dict = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** Normalize the clustering values using min-max normalization (0-1 range) to create a color gradient of the clustering value of each node for latter visualization. (Check [cmap](https://matplotlib.org/stable/users/explain/colors/colormaps.html) from matplotlib for more information.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "min_clustering : float = ...\n",
    "max_clustering : float = ...\n",
    "normalized_clustering : dict = ... # key = node_ids, values = normalized clustering values\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of clustering coefficients on the graph\n",
    "cmap = plt.cm.Blues\n",
    "color_mapping = {node_id: cmap(norm_clustering) for node_id,norm_clustering in normalized_clustering.items()}\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw_networkx(simpler_ny_graph,pos,arrows=False,ax=ax,node_color=list(color_mapping.values()),node_size=5,with_labels=False)\n",
    "fig.suptitle('Clustering coefficients over Manhattan Roads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** We can see that most of the nodes are homogeneous with a low clustering coefficient. Was this expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important notion in graph is the notion of [centrality](https://en.wikipedia.org/wiki/Centrality) for nodes in a graph. In short, it can be summarized as a set of functions that creates a ranking of each nodes based on a notion of importance in the graph. The latter is usually defined according to the application. In our case, we are interested in a road network and we can easily connect the notion of importance of a road to the one of shortest path. Indeed, a road that belongs to many shortest paths in the graph is very important for the flow and connectivity of the graph. Let's see how it works on our road dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin our analysis with the [betweenness centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.betweenness_centrality.html#networkx.algorithms.centrality.betweenness_centrality). It is a measure of how central a node is in terms of shortest path. In other word, a node $v$ has a high centrality if many shortest paths connecting any two other nodes $(s,t)$ go trough $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import betweenness_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7** Compute the betweenness centrality for all nodes in the graph. _(Hint: set `normalized=False` in the function arguments to avoid latter scaling issues when we will apply the color map.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "b_centrality : dict = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these centrality values (It is always good practice to have an idea of the kind of post-processing you want to apply on them later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot centrality histogram\n",
    "fig,ax = plt.subplots(figsize=(6,3))\n",
    "ax.hist(b_centrality.values())\n",
    "ax.set_xlabel('Centrality Value')\n",
    "ax.set_ylabel('Number of Nodes')\n",
    "fig.suptitle('Histogram of betweenness centrality values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the centrality values seems to follow a log normal distribution that we can normalize to have a better visual representation of the scale of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.8** Transform the centrality values using log scaling and z-normalization. _(Hint: Do not forget to add a small terms $\\epsilon<<0$ in the log function to avoid evaluation of zero values.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "log_centrality: dict = ... # key = node_ids, values = log centrality values\n",
    "mean_centrality : np.array = ...\n",
    "std_centrality : np.array = ...\n",
    "normalized_centrality : dict = ... # key = node_ids, values = normalized centrality values\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of centrality coefficients on the graph\n",
    "cmap = plt.cm.Blues\n",
    "color_mapping = {node_id: cmap(norm_centrality) for node_id,norm_centrality in normalized_centrality.items()}\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw_networkx(simpler_ny_graph,pos,arrows=False,ax=ax,node_color=list(color_mapping.values()),node_size=5,with_labels=False)\n",
    "fig.suptitle('Betweenness Centrality over Manhattan Roads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.9** Give a brief interpretation of the previous plot (why does nodes on the border of the graph have higher centrality? What happens around Central Park?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at another notion of centrality, the [closeness centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.closeness_centrality.html#networkx.algorithms.centrality.closeness_centrality). This metric is also connected to the notion of shortest path. Here, a node $v$ has high centrality if the average shortest path distance to any other nodes is low (meaning that we can quickly reach any other nodes $u$ in the network starting from $v$.) More formally, it is defined as $C(v)=\\frac{n-1}{\\sum_{u=1}^{n-1}d(v,u)}$, where $d(v,u)$ is the shortest path distance between nodes $v$ and $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.10** Compute the betweenness centrality for all nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import closeness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "c_centrality : dict = ...\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6,3))\n",
    "ax.hist(c_centrality.values())\n",
    "ax.set_xlabel('Centrality Value')\n",
    "ax.set_ylabel('Number of Nodes')\n",
    "fig.suptitle('Histogram of closeness centrality values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks again like a known distribution (Gaussian in this case). So let's normalize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.11** Transform the centrality values using z-normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here ###########################################################\n",
    "mean_centrality : np.array = ...\n",
    "std_centrality : np.array = ...\n",
    "normalized_centrality : dict = ... # key = node_ids, values = normalized centrality values\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of centrality coefficients on the graph\n",
    "cmap = plt.cm.Blues\n",
    "color_mapping = {node_id: cmap(norm_centrality) for node_id,norm_centrality in normalized_centrality.items()}\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw_networkx(simpler_ny_graph,pos,arrows=False,ax=ax,node_color=list(color_mapping.values()),node_size=5,with_labels=False)\n",
    "fig.suptitle('Closeness Centrality over Manhattan Roads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.12** Give a brief interpretation of the previous plot (why does the centrality follows this vertical (south-north) distribution over roads?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
